# Model
model_name_or_path: "./Qwen2.5-1.5B-Instruct/"
model_type: "llm_decoder"
num_labels: 1


# Dataset
train_dataset: "../../../example_data/t2rank_100.jsonl"
val_dataset: "../../../example_data/t2rank_100.small.jsonl"
max_label: 1
min_label: 0
max_len: 512

query_format: "query: {}"
document_format: "document: {}"
seq: "\n"
special_token: "</s>"




# Training
output_dir: "./output/t2ranking_100_example_llm"

## Model Saving
save_on_epoch_end: 1
num_max_checkpoints: 5

## Hyperparameters
loss_type: "point_ce"  # "point_ce" or "point_mse"
epochs: 2
lr: 5e-5
batch_size: 12
seed: 666
warmup_proportion: 0.1
gradient_accumulation_steps: 2
mixed_precision: bf16 


## Logging
log_interval: 10
log_with: "wandb" # "wandb" or "tensorboard"



