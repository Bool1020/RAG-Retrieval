# Model
model_name_or_path: "/data_train/search/zengziyang/models/Qwen/Qwen2.5-7B-Instruct"
## Model Max Input Length
max_len: 512 
model_type: "SeqClassificationRanker"
## SeqClassificationRanker features
num_labels: 1
### Model Input Format
query_format: "query: {}"
document_format: "document: {}"
seq: " "
special_token: "<score>"



# Dataset
train_dataset: "../../../example_data/t2rank_100.jsonl"
val_dataset: "../../../example_data/t2rank_100.small.jsonl"
max_label: 1
min_label: 0


# Training
output_dir: "./output/t2ranking_100_example"
## Hyperparameters
loss_type: "point_ce"  # "point_ce" or "point_mse"
epochs: 2
lr: 5e-5
batch_size: 2
seed: 666
warmup_proportion: 0.1
gradient_accumulation_steps: 4
### The gradient_accumulation_steps configuration in the deepspeed config file will override `gradient_accumulation_steps`
mixed_precision: bf16 
### `mixed_precision` setting other than None will overwrite the mixed_precision configuration in the deepspeed config file
## Model Saving
save_on_epoch_end: 0
num_max_checkpoints: 5


## Logging
log_interval: 10
log_with: "wandb" # "wandb" or "tensorboard"



