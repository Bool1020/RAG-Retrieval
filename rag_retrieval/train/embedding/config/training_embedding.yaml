# Model
model_name_or_path: "BAAI/bge-base-zh-v1.5" #or Alibaba-NLP/gte-Qwen2-7B-instruct
train_type: "train"


# Dataset
train_dataset: "../../../example_data/t2rank_100.jsonl"
neg_nums: 15
query_max_len: 128
passage_max_len: 512
shuffle: true


# Training
output_dir: "./output/t2ranking_100_example"
save_on_epoch_end: 1
num_max_checkpoints: 5



## Hyperparameters
temperature: 0.02
epochs: 2
lr: 2e-5
batch_size: 32
seed: 666
warmup_proportion: 0.1
gradient_accumulation_steps: 3
mixed_precision: fp16

##mrl
use_mrl: false
mrl_dims: "128, 256, 512, 768, 1024, 1280, 1536, 1792"



## Logging
log_interval: 10
log_with: "wandb" # "wandb" or "tensorboard"